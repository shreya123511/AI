{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pl94De0uLAxU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv(\"student.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 rows of dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nBottom 5 rows of dataset:\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCnKH40pLtiD",
        "outputId": "31641cfd-a83d-41bb-f713-d75f71b216ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Information:\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXLews7vMAmf",
        "outputId": "dca179ae-b834-4179-f870-9c6f56cf3277"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Descriptive Statistics:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIexVa-iMoFF",
        "outputId": "98a4608b-2c47-4dfb-de77-f7194490e81c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into Features (X) and Label (Y)\n",
        "X = data[['Math', 'Reading']].values   # Features: Math and Reading scores\n",
        "Y = data['Writing'].values             # Label: Writing score\n",
        "\n",
        "# Display first few rows of X and Y\n",
        "print(\"\\nFeature Matrix (X):\")\n",
        "print(X[:5])\n",
        "\n",
        "print(\"\\nLabel Vector (Y):\")\n",
        "print(Y[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WD0dNr0NJap",
        "outputId": "c4e060e6-7420-4d3a-aad9-63d550c7a334"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Matrix (X):\n",
            "[[48 68]\n",
            " [62 81]\n",
            " [79 80]\n",
            " [76 83]\n",
            " [59 64]]\n",
            "\n",
            "Label Vector (Y):\n",
            "[63 72 78 79 62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"student.csv\")\n",
        "\n",
        "# Extract features (Math, Reading) and label (Writing)\n",
        "features = data[['Math', 'Reading']].to_numpy()\n",
        "labels = data['Writing'].to_numpy()\n",
        "\n",
        "# Create matrices in required form\n",
        "X = features.T\n",
        "Y = labels\n",
        "W = np.zeros(X.shape[0])\n",
        "\n",
        "print(\"Feature Matrix X (d x n):\")\n",
        "print(X[:, :5])\n",
        "\n",
        "print(\"\\nWeight Vector W (d):\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nLabel Vector Y (n):\")\n",
        "print(Y[:5])\n",
        "\n",
        "# Prediction rule (no bias term)\n",
        "Y_pred = W @ X\n",
        "print(\"\\nPredicted Y (first 5):\")\n",
        "print(Y_pred[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaIDYeH9Op0Y",
        "outputId": "42c550b8-f1c9-4644-8789-a0e216db1eba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix X (d x n):\n",
            "[[48 62 79 76 59]\n",
            " [68 81 80 83 64]]\n",
            "\n",
            "Weight Vector W (d):\n",
            "[0. 0.]\n",
            "\n",
            "Label Vector Y (n):\n",
            "[63 72 78 79 62]\n",
            "\n",
            "Predicted Y (first 5):\n",
            "[0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n",
        "\n",
        "# Shuffle indices for randomness\n",
        "n = len(X)\n",
        "indices = np.arange(n)\n",
        "np.random.seed(42)          # reproducibility\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# 80-20 Split\n",
        "train_size_80 = int(0.8 * n)\n",
        "train_idx_80, test_idx_20 = indices[:train_size_80], indices[train_size_80:]\n",
        "\n",
        "X_train_80, X_test_20 = X[train_idx_80], X[test_idx_20]\n",
        "Y_train_80, Y_test_20 = Y[train_idx_80], Y[test_idx_20]\n",
        "\n",
        "print(\"80-20 Split:\")\n",
        "print(\"Training set size:\", X_train_80.shape[0])\n",
        "print(\"Test set size:\", X_test_20.shape[0])\n",
        "\n",
        "# 70-30 Split\n",
        "train_size_70 = int(0.7 * n)\n",
        "train_idx_70, test_idx_30 = indices[:train_size_70], indices[train_size_70:]\n",
        "\n",
        "X_train_70, X_test_30 = X[train_idx_70], X[test_idx_30]\n",
        "Y_train_70, Y_test_30 = Y[train_idx_70], Y[test_idx_30]\n",
        "\n",
        "print(\"\\n70-30 Split:\")\n",
        "print(\"Training set size:\", X_train_70.shape[0])\n",
        "print(\"Test set size:\", X_test_30.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y4toD2tQC7f",
        "outputId": "764a6ede-411b-4da2-e339-7410f4306453"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80-20 Split:\n",
            "Training set size: 800\n",
            "Test set size: 200\n",
            "\n",
            "70-30 Split:\n",
            "Training set size: 700\n",
            "Test set size: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X: Feature Matrix (d x n)\n",
        "    Y: Target Vector (n,)\n",
        "    W: Weight Vector (d,)\n",
        "\n",
        "    Returns:\n",
        "    cost: Mean Squared Error (MSE) with 1/(2n) scaling\n",
        "    \"\"\"\n",
        "    n = X.shape[1]                     # number of samples\n",
        "    Y_pred = W @ X                     # hypothesis hθ(X) = W^T X\n",
        "    errors = Y_pred - Y                # difference between prediction and actual\n",
        "    cost = (1 / (2 * n)) * np.sum(errors ** 2)\n",
        "    return cost\n",
        "# Given matrices\n",
        "X = np.array([[1, 3, 5],\n",
        "              [2, 4, 6]])\n",
        "Y = np.array([3, 7, 11])\n",
        "W = np.array([1, 1])\n",
        "\n",
        "# Compute cost\n",
        "cost_value = cost_function(X, Y, W)\n",
        "print(\"Computed Cost:\", cost_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygYXPu7CQ2Bk",
        "outputId": "ef831856-b629-41ac-da18-e9b3d5f3f05e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Cost: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    n = X.shape[0]\n",
        "    Y_pred = X @ W\n",
        "    errors = Y_pred - Y\n",
        "    cost = (1 / (2 * n)) * np.sum(errors ** 2)\n",
        "    return cost\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2],\n",
        "                   [3, 4],\n",
        "                   [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhPYD3ulRjVa",
        "outputId": "be137d9d-e995-4740-ee8b-7b540935520e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, alpha=0.01, epochs=1000):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to learn weights W.\n",
        "    Assumes no bias term (b = 0).\n",
        "\n",
        "    Parameters:\n",
        "    X: Feature matrix (m x d)\n",
        "    Y: Target vector (m,)\n",
        "    alpha: Learning rate\n",
        "    epochs: Number of iterations\n",
        "\n",
        "    Returns:\n",
        "    W: Learned weight vector (d,)\n",
        "    \"\"\"\n",
        "    m, d = X.shape\n",
        "    W = np.zeros(d)   # initialize weights\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        Y_pred = X @ W                 # predictions\n",
        "        errors = Y_pred - Y            # loss\n",
        "        gradient = (1/m) * (X.T @ errors)  # derivative wrt W\n",
        "        W -= alpha * gradient          # update rule\n",
        "\n",
        "    return W\n",
        "\n",
        "# Example run with the test dataset\n",
        "W_learned = gradient_descent(X_test, Y_test, alpha=0.01, epochs=1000)\n",
        "print(\"Learned Weights:\", W_learned)\n"
      ],
      "metadata": {
        "id": "kFlfl4kmR-By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, Y, alpha=0.01, epochs=1000):\n",
        "    m, d = X.shape          # m = samples, d = features\n",
        "    W = np.zeros(d)         # start with weights = 0\n",
        "\n",
        "    for _ in range(epochs): # repeat many times\n",
        "        Y_pred = X @ W      # predict values\n",
        "        loss = Y_pred - Y   # error\n",
        "        gradient = (1/m) * (X.T @ loss)  # slope\n",
        "        W = W - alpha * gradient         # update weights\n",
        "\n",
        "    return W\n",
        "\n",
        "X_test = np.array([[1, 2],\n",
        "                   [3, 4],\n",
        "                   [5, 6]])   # features\n",
        "Y_test = np.array([3, 7, 11]) # targets\n",
        "\n",
        "W_learned = gradient_descent(X_test, Y_test, alpha=0.01, epochs=1000)\n",
        "print(\"Learned Weights:\", W_learned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLkuG3CxSY-X",
        "outputId": "720d86f3-ede0-4eb8-9fbf-e9cc0d9451f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Weights: [0.9463076  1.04238709]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost function (from To-Do-4)\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = X @ W\n",
        "    errors = Y_pred - Y\n",
        "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent Implementation\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n)\n",
        "    Y (numpy.ndarray): Target vector (m,)\n",
        "    W (numpy.ndarray): Initial guess for parameters (n,)\n",
        "    alpha (float): Learning rate\n",
        "    iterations (int): Number of iterations\n",
        "\n",
        "    Returns:\n",
        "    W_update (numpy.ndarray): Updated parameters (n,)\n",
        "    cost_history (list): History of cost values over iterations\n",
        "    \"\"\"\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = X @ W\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * (X.T @ loss)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W = W - alpha * dw\n",
        "        W_update = W\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history\n"
      ],
      "metadata": {
        "id": "_yQTRJeUTCCO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost function\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = X @ W\n",
        "    errors = Y_pred - Y\n",
        "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis\n",
        "        Y_pred = X @ W\n",
        "\n",
        "        # Step 2: Loss\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient\n",
        "        dw = (1 / m) * (X.T @ loss)\n",
        "\n",
        "        # Step 4: Update weights\n",
        "        W = W - alpha * dw\n",
        "        W_update = W\n",
        "\n",
        "        # Step 5: Cost\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Test Case\n",
        "np.random.seed(0)  # reproducibility\n",
        "X = np.random.rand(100, 3)   # 100 samples, 3 features\n",
        "Y = np.random.rand(100)      # targets\n",
        "W = np.random.rand(3)        # initial weights\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Final Cost:\", cost_history[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxspBR3MTvvv",
        "outputId": "d6c07242-1a2f-4fad-a945-c8414e25918d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Final Cost: 0.05435492255484332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Root Mean Square Error (RMSE)\n",
        "\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Actual target values\n",
        "    Y_pred (numpy.ndarray): Predicted values\n",
        "\n",
        "    Returns:\n",
        "    float: RMSE value\n",
        "    \"\"\"\n",
        "    n = len(Y)\n",
        "    return np.sqrt(np.sum((Y - Y_pred) ** 2) / n)\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)   # 100 samples, 3 features\n",
        "Y = np.random.rand(100)      # targets\n",
        "W = np.random.rand(3)        # initial weights\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Predictions with learned weights\n",
        "Y_pred = X @ final_params\n",
        "\n",
        "# Evaluate RMSE\n",
        "rmse_value = rmse(Y, Y_pred)\n",
        "print(\"RMSE:\", rmse_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxlHKY2IUcCr",
        "outputId": "7c9b3359-cc66-4250-a00c-3953d8938c0c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.32971176064812524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates how far off the predictions are from the actual values on average.\n",
        "\n",
        "    Parameters:\n",
        "    Y (array): Actual target values (what really happened)\n",
        "    Y_pred (array): Predicted values from the model\n",
        "\n",
        "    Returns:\n",
        "    float: RMSE value — lower means better predictions\n",
        "    \"\"\"\n",
        "    n = len(Y)\n",
        "    error = Y - Y_pred\n",
        "    rmse = np.sqrt(np.sum(error**2) / n)\n",
        "    return rmse\n",
        "# test\n",
        "Y = np.array([3, 7, 11])\n",
        "Y_pred = np.array([2.8, 7.2, 10.9])\n",
        "print(\"RMSE:\", rmse(Y, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf5ELMr1U1af",
        "outputId": "7d6f6190-12ae-46ef-a77b-e0a884684019"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.17320508075688779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def r2_score(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate R-squared (coefficient of determination) to check how well\n",
        "    the regression model explains the variation in the actual data.\n",
        "\n",
        "    R² tells us the proportion of the variance in the target values (Y)\n",
        "    that can be explained by the predictions (Y_pred).\n",
        "    - R² = 1 means perfect fit (predictions match actual values exactly).\n",
        "    - R² = 0 means the model does not explain any variation.\n",
        "    - Negative R² means the model performs worse than just predicting the mean.\n",
        "\n",
        "    Parameters:\n",
        "    Y (array): Actual target values (what really happened)\n",
        "    Y_pred (array): Predicted values from the model\n",
        "\n",
        "    Returns:\n",
        "    float: R² value (closer to 1 means better fit)\n",
        "    \"\"\"\n",
        "    # Total variation in actual values (how far each value is from the mean)\n",
        "    sst = np.sum((Y - np.mean(Y))**2)\n",
        "\n",
        "    # Residual variation (how far each value is from its prediction)\n",
        "    ssr = np.sum((Y - Y_pred)**2)\n",
        "\n",
        "    # R² formula: 1 - (unexplained variation / total variation)\n",
        "    r2 = 1 - (ssr / sst)\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "4OCRfPcTVjiB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Model Evaluation - R²\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates how well the model's predictions match the actual values.\n",
        "\n",
        "    This function returns the R-squared score, which tells us how much of the variation\n",
        "    in the actual data is explained by the model. A score close to 1 means a good fit.\n",
        "\n",
        "    Parameters:\n",
        "    Y (array): Actual target values\n",
        "    Y_pred (array): Predicted values from the model\n",
        "\n",
        "    Returns:\n",
        "    float: R² score (closer to 1 means better fit)\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)                      # average of actual values\n",
        "\n",
        "    # Total variation in actual values (from the mean)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Unexplained variation (from predictions)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R² formula\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "# test\n",
        "Y = np.array([3, 7, 11])\n",
        "Y_pred = np.array([2.8, 7.2, 10.9])\n",
        "print(\"R² Score:\", r2(Y, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cop5IqUV-ff",
        "outputId": "e8f544d3-82de-4c0d-82bc-333ff8bf8463"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score: 0.9971875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost function\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = X @ W\n",
        "    errors = Y_pred - Y\n",
        "    return (1 / (2 * m)) * np.sum(errors ** 2)\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = X @ W              # predictions\n",
        "        loss = Y_pred - Y           # error\n",
        "        dw = (1 / m) * (X.T @ loss) # slope\n",
        "        W = W - alpha * dw          # update weights\n",
        "        cost_history.append(cost_function(X, Y, W))\n",
        "    return W, cost_history\n",
        "\n",
        "# RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    n = len(Y)\n",
        "    return np.sqrt(np.sum((Y - Y_pred) ** 2) / n)\n",
        "\n",
        "# R²\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)   # total variation\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)   # error variation\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function\n",
        "def run_linear_regression():\n",
        "    np.random.seed(0)\n",
        "    X = np.random.rand(100, 3)   # 100 samples, 3 features\n",
        "    Y = np.random.rand(100)      # actual values\n",
        "    W_init = np.random.rand(3)   # starting weights\n",
        "\n",
        "    # Set learning rate and iterations\n",
        "    alpha = 0.01\n",
        "    iterations = 1000\n",
        "\n",
        "    # Train model\n",
        "    final_W, cost_history = gradient_descent(X, Y, W_init, alpha, iterations)\n",
        "\n",
        "    # Predictions\n",
        "    Y_pred = X @ final_W\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Final Weights:\", final_W)\n",
        "    print(\"Final Cost:\", cost_history[-1])\n",
        "    print(\"RMSE:\", rmse(Y, Y_pred))\n",
        "    print(\"R² Score:\", r2(Y, Y_pred))\n",
        "\n",
        "run_linear_regression()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvGOn5dYW4ZB",
        "outputId": "12d74745-d223-462a-8f57-e0f925673f8f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.20551667 0.54295081 0.10388027]\n",
            "Final Cost: 0.05435492255484332\n",
            "RMSE: 0.32971176064812524\n",
            "R² Score: -0.34175367492079367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Gradient Descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = X @ W              # predictions\n",
        "        loss = Y_pred - Y           # error\n",
        "        dw = (1 / m) * (X.T @ loss) # slope\n",
        "        W = W - alpha * dw          # update weights\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)  # cost function\n",
        "        cost_history.append(cost)\n",
        "    return W, cost_history\n",
        "\n",
        "# RMSE function\n",
        "def rmse(Y, Y_pred):\n",
        "    n = len(Y)\n",
        "    return np.sqrt(np.sum((Y - Y_pred) ** 2) / n)\n",
        "\n",
        "# R² function\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)   # total variation\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)   # error variation\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    data = pd.read_csv(\"student.csv\")\n",
        "    # Prepare features (X) and target (Y)\n",
        "    X = data[[\"Math\", \"Reading\"]].values   # inputs: Math & Reading marks\n",
        "    Y = data[\"Writing\"].values\n",
        "\n",
        "    # Split into training (80%) and test (20%)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize weights and hyperparameters\n",
        "    W = np.zeros(X_train.shape[1])   # start with zeros\n",
        "    alpha = 0.00001                  # learning rate\n",
        "    iterations = 1000                # number of steps\n",
        "\n",
        "    # Train model using Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", rmse(Y_test, Y_pred))\n",
        "    print(\"R² on Test Set:\", r2(Y_test, Y_pred))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyEoF5AWYAs8",
        "outputId": "fcbd5788-eba2-481a-e951-d9e1cf485e4f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10): [np.float64(2471.69875), np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.479489285058), np.float64(889.9583270083235), np.float64(726.8940993009545), np.float64(594.2897260808594), np.float64(486.4552052951634), np.float64(398.7634463599482)]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R² on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    }
  ]
}